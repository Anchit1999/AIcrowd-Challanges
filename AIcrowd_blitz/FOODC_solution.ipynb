{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FOODC_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97a2b3e61dfc462d922ae44b194090b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b8a15aa710c4b30aa9f0f9a9248809c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3f3e29e6a574b8fbf2076395cdfee7f",
              "IPY_MODEL_dd13636c99bb44eea4d06edea9aa073f"
            ]
          }
        },
        "7b8a15aa710c4b30aa9f0f9a9248809c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3f3e29e6a574b8fbf2076395cdfee7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b50096a851264f16b3fb8ad68406930b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 91675053,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 91675053,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1d457884b024a078600ebc9dda3ebfa"
          }
        },
        "dd13636c99bb44eea4d06edea9aa073f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9331b705926c4d7d92af3843b4e77b4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87.4M/87.4M [04:17&lt;00:00, 356kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab904447068a4f78b138b154eefb08ae"
          }
        },
        "b50096a851264f16b3fb8ad68406930b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1d457884b024a078600ebc9dda3ebfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9331b705926c4d7d92af3843b4e77b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab904447068a4f78b138b154eefb08ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tBZDUGlE0PBv"
      },
      "source": [
        "# FOODC Densenet 161 LB 0.64\n",
        "\n",
        "Author - Hard_drive_corrupted (Pulkit Gera & Anchit Gupta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFLEqstQw7qO",
        "colab_type": "text"
      },
      "source": [
        "## To open this notebook on Google Computing platform Colab, click below!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DaiLz4kw7qU",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/aicrowd-bot/8a3e4b475f70e48ee7d5dfa168997073)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbEPmjRBruLT",
        "colab_type": "text"
      },
      "source": [
        "##Solution\n",
        "We apply basic augmentation first to the dataset. After that we do the following\n",
        "+ We train the model on image sizes 128x128 for few epochs \n",
        "+ We train again for a few more epochs on image sizes 256x256\n",
        "+ We apply TTA to improve the solution\n",
        "\n",
        "We use Densenet 161 with SGD optimizer and Step LR scheduler for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3I76RK360zoY"
      },
      "source": [
        "## Download the files\n",
        "These include the train test images as well the csv indexing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0n8XZKR90UJg",
        "colab": {}
      },
      "source": [
        "!wget -q https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/foodc/v0.1/train_images.zip\n",
        "!wget -q https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/foodc/v0.1/test_images.zip\n",
        "!wget -q https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/foodc/v0.1/train.csv\n",
        "!wget -q https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/foodc/v0.1/test.csv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jyyJhW_i08vu"
      },
      "source": [
        "We create directories and unzip the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9XfPwVM2GoH",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/test\n",
        "!mkdir data/train\n",
        "!unzip train_images -d data/train\n",
        "!unzip test_images -d data/test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LzTnvAlm1CSJ"
      },
      "source": [
        "## Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "82DFmOJZ5mF5",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCnI6hIU5Jah"
      },
      "source": [
        "## Loading Data\n",
        "In pytorch we can directly load our files into torchvision(the library which creates the object) or create a custom class to load data. The class must have `__init__` , `__len__` and `__getitem__` functions. We create a custom dataloader to suit our needs. More info on custom loaders can be read [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BEEA7ixs5U91",
        "colab": {}
      },
      "source": [
        "class FoodData(Dataset):\n",
        "    def __init__(self,data_list,data_dir = './',transform=None,train=True):\n",
        "        super().__init__()\n",
        "        self.data_list = data_list\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_list.shape[0]\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        if self.train:\n",
        "          img_name,label = self.data_list.iloc[item]\n",
        "        else:\n",
        "          img_name = self.data_list.iloc[item]['ImageId']\n",
        "        img_path = os.path.join(self.data_dir,img_name)\n",
        "        img = cv2.imread(img_path,1)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img,(256,256))\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.train:\n",
        "          return {\n",
        "              'gt' : img,\n",
        "              'label' : torch.tensor(label)\n",
        "\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "              'gt':img\n",
        "          }\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uSPuLT_v4znK"
      },
      "source": [
        "We first convert the data labels into encodings using Label Encoders. This basically converts labels into number encodings. This is an important step as without it we cannot train our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XOo5Q96onPVW",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "le = preprocessing.LabelEncoder()\n",
        "targets = le.fit_transform(train['ClassName'])\n",
        "ntrain = train\n",
        "ntrain['ClassName'] = targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5XRsFYoMzfm",
        "colab_type": "code",
        "outputId": "0a6cff98-3f1e-489b-aa75-fa2c81568a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "ntrain"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageId</th>\n",
              "      <th>ClassName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f27632d7e5.jpg</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>efa87919ed.jpg</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4f169e8c8d.jpg</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a6956654bf.jpg</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d99ce8c3bf.jpg</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9318</th>\n",
              "      <td>ba8233c7d2.jpg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9319</th>\n",
              "      <td>2090043907.jpg</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9320</th>\n",
              "      <td>8762d1cefd.jpg</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9321</th>\n",
              "      <td>28e7439245.jpg</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9322</th>\n",
              "      <td>ba263cfb41.jpg</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9323 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             ImageId  ClassName\n",
              "0     f27632d7e5.jpg         55\n",
              "1     efa87919ed.jpg         41\n",
              "2     4f169e8c8d.jpg         12\n",
              "3     a6956654bf.jpg         44\n",
              "4     d99ce8c3bf.jpg         23\n",
              "...              ...        ...\n",
              "9318  ba8233c7d2.jpg          7\n",
              "9319  2090043907.jpg         58\n",
              "9320  8762d1cefd.jpg         14\n",
              "9321  28e7439245.jpg         12\n",
              "9322  ba263cfb41.jpg         21\n",
              "\n",
              "[9323 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3EYWQlhe5w_I"
      },
      "source": [
        "We load our train data and some necessary augementations like converting to PIL image, converting to tensors and normalizing them across channels. We can add more augementations such as `Random Flip`, `Random Rotation`, etc more on which can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BBwgTv6v-JjC",
        "colab": {}
      },
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize( mean = np.array([0.485, 0.456, 0.406]),\n",
        "    std = np.array([0.229, 0.224, 0.225]))\n",
        "])\n",
        "train_path = 'data/train/train_images'\n",
        "train_data = FoodData(data_list= ntrain,data_dir = train_path,transform = transforms_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nv98vHXGtTVZ"
      },
      "source": [
        "## EDA\n",
        "Let us do some exploratory data analysis. The idea is to see the class distribution, how the images are and much more. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aEaBh0je6NY5"
      },
      "source": [
        "## Split Data into Train and Validation\n",
        "Now we want to see how well our model is performing, but we dont have the test data labels with us to check. What do we do ? So we split our dataset into train and validation. The idea is that we test our classifier on validation set in order to get an idea of how well our classifier works. This way we can also ensure that we dont [overfit](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) on the train dataset. There are many ways to do validation like [k-fold](https://machinelearningmastery.com/k-fold-cross-validation/),[leave one out](https://en.wikipedia.org/wiki/Cross-validation_(statistics), etc  \n",
        "\n",
        "We also make `dataloaders` which basically create minibatches of dataset which are used in each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KQx0SjTzC8rD",
        "colab": {}
      },
      "source": [
        "batch = 4\n",
        "valid_size = 0.2\n",
        "num = train_data.__len__()\n",
        "# Dividing the indices for train and cross validation\n",
        "indices = list(range(num))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num))\n",
        "train_idx,valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "#Create Samplers\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = batch, sampler = train_sampler)\n",
        "valid_loader = DataLoader(train_data, batch_size = batch, sampler = valid_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "daFAO0Ez6qxv"
      },
      "source": [
        "Here we load test images. Note: This file will not have any labels with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ozAU2IdEDzCl",
        "colab": {}
      },
      "source": [
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize( mean = np.array([0.485, 0.456, 0.406]),\n",
        "    std = np.array([0.229, 0.224, 0.225]))\n",
        "])\n",
        "test_path = 'data/test/test_images'\n",
        "test = pd.read_csv('test.csv')\n",
        "test_data = FoodData(data_list= test,data_dir = test_path,transform = transforms_test,train=False)\n",
        "\n",
        "test_loader = DataLoader(test_data, batch_size=batch, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZWfGdah604o"
      },
      "source": [
        "Here we check if we have a GPU or not. If we have we just need to shift our data and model to GPU for faster computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l_W_9ZtdD1Mb",
        "outputId": "50ea86cc-19b2-4e13-87ef-5a03664a9a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y5XRek7969EM"
      },
      "source": [
        "## Define the Model\n",
        "Now we come to the juicy part. We define our model here. We need to create a class with `__init__` and `forward` functions which define the layers and forward pass respectively. We can also load pretrained models and freeze their layers and add more layers on top of it, to train them. More on pretrained models with pytorch [here](https://pytorch.org/docs/stable/torchvision/models.html) and making models [here](https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQTllKAtBJ_e",
        "colab_type": "code",
        "outputId": "15027cca-07d0-463e-a65a-a6a48dd7f3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_sampler.__len__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7459"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOAXdZoF4Z6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloaders = {}\n",
        "dataset_sizes = {}\n",
        "dataloaders['train'] = train_loader\n",
        "dataloaders['val'] = valid_loader\n",
        "dataset_sizes['train'] = train_sampler.__len__()\n",
        "dataset_sizes['val'] = valid_sampler.__len__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_edK9kZRa6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "\n",
        "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0.:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index,:]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(y_a, y_b, lam):\n",
        "    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb60WYm04A7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in tqdm(dataloaders[phase], position=0, leave=True):\n",
        "                inputs = data['gt'].squeeze(0).to(device)\n",
        "                labels = data['label'].to(device)\n",
        "                  \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)                    \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # loss = criterion(outputs, labels)\n",
        "                    loss = criterion(outputs,labels) \n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # running_corrects += torch.sum(preds == labels.data)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'best_model_so_far.pth')\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fpf-uJRFikW",
        "colab_type": "code",
        "outputId": "51ac8d6c-8206-4536-df8a-2c80efe9fd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "97a2b3e61dfc462d922ae44b194090b6",
            "7b8a15aa710c4b30aa9f0f9a9248809c",
            "c3f3e29e6a574b8fbf2076395cdfee7f",
            "dd13636c99bb44eea4d06edea9aa073f",
            "b50096a851264f16b3fb8ad68406930b",
            "a1d457884b024a078600ebc9dda3ebfa",
            "9331b705926c4d7d92af3843b4e77b4e",
            "ab904447068a4f78b138b154eefb08ae"
          ]
        }
      },
      "source": [
        "import pretrainedmodels\n",
        "model_name = 'xception' # could be fbresnet152 or inceptionresnetv2\n",
        "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth\" to /root/.cache/torch/checkpoints/xception-43020ad28.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97a2b3e61dfc462d922ae44b194090b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=91675053.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC6hQmag4C3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "model_ft = models.densenet161(pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 61)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOSUCTON6CFF",
        "colab_type": "code",
        "outputId": "31980f05-dd5a-4501-914f-b854d282952a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1865 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1865/1865 [21:56<00:00,  1.42it/s]\n",
            "  0%|          | 0/466 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 2.7353 Acc: 0.3289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 466/466 [01:55<00:00,  4.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 1.8403 Acc: 0.5150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1865 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1865/1865 [21:56<00:00,  1.42it/s]\n",
            "  0%|          | 0/466 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 2.0026 Acc: 0.4699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 466/466 [01:55<00:00,  4.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 1.7581 Acc: 0.5370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1865 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1865/1865 [21:55<00:00,  1.42it/s]\n",
            "  0%|          | 0/466 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.7405 Acc: 0.5251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 466/466 [01:55<00:00,  4.04it/s]\n",
            "  0%|          | 0/1865 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 1.8421 Acc: 0.5338\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1865/1865 [21:54<00:00,  1.42it/s]\n",
            "  0%|          | 0/466 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.6055 Acc: 0.5546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 466/466 [01:55<00:00,  4.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 1.7591 Acc: 0.5756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1865 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1865/1865 [21:55<00:00,  1.42it/s]\n",
            "  0%|          | 0/466 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.4725 Acc: 0.5789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 466/466 [01:54<00:00,  4.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 1.5856 Acc: 0.5810\n",
            "\n",
            "Training complete in 119m 15s\n",
            "Best val Acc: 0.581009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1W5G95cSTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QCppJXybBvqq"
      },
      "source": [
        "## Predict on Validation\n",
        "Now we predict our trained model on the validation set and evaluate our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYUSVSY6vY4m",
        "colab_type": "code",
        "outputId": "65a2e33e-0dd7-4b94-cd6a-bfe57a8c07ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!pip install ttach"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ttach\n",
            "  Downloading https://files.pythonhosted.org/packages/53/22/470bb42f90505dc572f6bbcf3ac84d67aaf1554cd48cc08f788c36fec129/ttach-0.0.2-py3-none-any.whl\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhYg50Xkdyzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ttach as tta\n",
        "transforms = tta.Compose(\n",
        "    [\n",
        "\t# tta.FiveCrops(128,128),        \t\n",
        "\ttta.HorizontalFlip(),\n",
        "\ttta.VerticalFlip(),\n",
        "\t# tta.FiveCrops(128,128),\n",
        "  tta.Rotate90(angles=[0, 90]),\n",
        "        #tta.Scale(scales=[1, 2, 4]),\n",
        "\t        \n",
        "    ]\n",
        ")\n",
        "model_ft.load_state_dict(torch.load('best_model_so_far.pth'))\n",
        "model_ft.eval()\n",
        "tta_model = tta.ClassificationTTAWrapper(model_ft, transforms,merge_mode='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aFau5IeyG3c3",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "pred_list = []\n",
        "correct_list = []\n",
        "with torch.no_grad():\n",
        "    for images in tqdm(valid_loader):\n",
        "        data = images['gt'].squeeze(0).to(device)\n",
        "        target = images['label'].to(device)\n",
        "        outputs = tta_model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        pr = predicted.detach().cpu().numpy()\n",
        "        for i in pr:\n",
        "          pred_list.append(i)\n",
        "        tg = target.detach().cpu().numpy()\n",
        "        for i in tg:\n",
        "          correct_list.append(i)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AHhrW6WZkG1P"
      },
      "source": [
        "## Evaluate the Performance\n",
        "We use the same metrics as that will be used for the test set.  \n",
        "[F1 score](https://en.wikipedia.org/wiki/F1_score) and [Log Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) are the metrics for this challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "167pHkZTKshk",
        "outputId": "b798f35e-a828-4900-c849-5d69068d13eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score,precision_score,log_loss   \n",
        "print(\"F1 score :\",f1_score(correct_list,pred_list,average='micro')*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score : 65.71888412017167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BCP85OMQDE3C"
      },
      "source": [
        "## Predict on test set\n",
        "Time for the moment of truth! Predict on test set and time to make the submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qE19LWBnhowq",
        "colab": {}
      },
      "source": [
        "model_ft.load_state_dict(torch.load('best_model_so_far.pth'))\n",
        "model_ft.eval()\n",
        "\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for images in tqdm(test_loader):\n",
        "        data = images['gt'].squeeze(0).to(device)\n",
        "        outputs = model_ft(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        pr = predicted.detach().cpu().numpy()\n",
        "        for i in pr:\n",
        "          preds.append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9nCB-d-yDUEc"
      },
      "source": [
        "## Save it in correct format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGiuketcDOYg",
        "colab": {}
      },
      "source": [
        "# Create Submission file        \n",
        "df = pd.DataFrame(le.inverse_transform(preds),columns=['ClassName'])\n",
        "df.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nW0Eh62ZDV94"
      },
      "source": [
        "## To download the generated in collab csv run the below command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aL36neEfrvcO",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RM_Rkc5UDYjD"
      },
      "source": [
        "### Go to [platform](https://www.aicrowd.com/challenges/aicrowd-blitz-may-2020/problems/foodc). Participate in the challenge and submit the submission.csv."
      ]
    }
  ]
}